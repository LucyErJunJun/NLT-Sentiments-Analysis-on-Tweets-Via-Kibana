{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Near Real Time Prediction On Sentiment Of Tweets With Elasticsearch And Kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple demo of near realtime sentiment analysis on Tweets using Elasticsearch, Kibana and Twitter API in Python 3\n",
    "\n",
    "Interested in o president Trump's perfomance ten month after his inauguration, I use Twitter public stream to analyze people's sentiment on topics mentioned his name in a near rea-time way. In this demo, I will show how to pipe data from Twitter, predict the sentiment with Textblob, inject **Hot Data** to ElasticSeach using Twitter APIs and, finally, present the output on Kibana Dashboard.\n",
    "\n",
    "The following screenshot is one example of showing poeple's sentiment to president Trump's role in Topics (Twitter Hashtag) on the Kibana dashboard. To access the the animation, please visit the [interaction with the Dashboard](https://www.dropbox.com/s/f8vhacnusdwghgb/RecordDashboard.webm?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./screencapture-localhost-5601-app-kibana-without%20Geo.png\" width='80%'>\n",
    "<img src=\"./RecordDashboard.gif\" width='80%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, sentiment of Tweets are predicted by [model](https://textblob.readthedocs.io/en/dev) trained, and the output are injected into the locally installed Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Mac users who'd like to install manually, please check [How to install Elasticsearch on mac os x](https://chartio.com/resources/tutorials/how-to-install-elasticsearch-on-mac-os-x/)\n",
    "You may add the enviroment variable to your bash profile as described in the webpage above or change directory to unzipped file and then, launch Elasticsearch by typing in 'bin/elasticsearch'in Terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Kibana to get the \"Hot Data\" visualized, please check [Hot Data Visualized](https://www.youtube.com/watch?v=psNH33pcGBo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Python Script to Pipe Tweets To Elasticsearch With Sentiment Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from textblob import TextBlob #predict the sentiment of Tweet, see 'https://textblob.readthedocs.io/en/dev/'\n",
    "from elasticsearch import Elasticsearch #pip install Elasticsearch if not intalled yet\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import numpy as np\n",
    "from http.client import IncompleteRead\n",
    "\n",
    "#log in to your Twitter Application Management to create an App, url: 'https://apps.twitter.com'\n",
    "consumer_key = '<Twitter_Consumer_Key>'\n",
    "consumer_secret = '<Twitter_Consumer_Secret>'\n",
    "access_token = '<Twitter_Access_Token>'\n",
    "access_token_secret = '<Twitter_Access_Token_Secret>'\n",
    "\n",
    "# create instance of elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "\n",
    "class TweetStreamListener(StreamListener):\n",
    "    \n",
    "    # re-write the on_data function in the TweetStreamListener\n",
    "    # This function enables more functions than 'on_status', see 'https://stackoverflow.com/questions/31054656/what-is-the-difference-between-on-data-and-on-status-in-the-tweepy-library'        \n",
    "    def on_data(self, data):\n",
    "        # To understand the key-values pulled from Twitter, see 'https://dev.twitter.com/overview/api/tweets'\n",
    "        dict_data = json.loads(data)\n",
    "        # pass Tweet into TextBlob to predict the sentiment\n",
    "        tweet = TextBlob(dict_data[\"text\"]) if \"text\" in dict_data.keys() else None\n",
    "        \n",
    "        # if the object contains Tweet\n",
    "        if tweet:\n",
    "            # determine if sentiment is positive, negative, or neutral\n",
    "            if tweet.sentiment.polarity < 0:\n",
    "                sentiment = \"negative\"\n",
    "            elif tweet.sentiment.polarity == 0:\n",
    "                sentiment = \"neutral\"\n",
    "            else:\n",
    "                sentiment = \"positive\"\n",
    "            \n",
    "            # print the predicted sentiment with the Tweets\n",
    "            print(sentiment, tweet.sentiment.polarity, dict_data[\"text\"])\n",
    "            \n",
    "    \n",
    "            # extract the first hashtag from the object\n",
    "            # transform the Hashtags into proper case\n",
    "            if len(dict_data[\"entities\"][\"hashtags\"])>0:\n",
    "                hashtags=dict_data[\"entities\"][\"hashtags\"][0][\"text\"].title()\n",
    "            else:\n",
    "                #Elasticeach does not take None object\n",
    "                hashtags=\"None\"\n",
    "                      \n",
    "            # add text and sentiment info to elasticsearch\n",
    "            es.index(index=\"logstash-a\",\n",
    "                     # create/inject data into the cluster with index as 'logstash-a'\n",
    "                     # create the naming pattern in Management/Kinaba later in order to push the data to a dashboard\n",
    "                     doc_type=\"test-type\",\n",
    "                     body={\"author\": dict_data[\"user\"][\"screen_name\"],\n",
    "                           \"followers\":dict_data[\"user\"][\"followers_count\"],\n",
    "                           #parse the milliscond since epoch to elasticsearch and reformat into datatime stamp in Kibana later\n",
    "                           \"date\": datetime.strptime(dict_data[\"created_at\"], '%a %b %d %H:%M:%S %z %Y'),\n",
    "                           \"message\": dict_data[\"text\"]  if \"text\" in dict_data.keys() else \" \",\n",
    "                           \"hashtags\":hashtags,\n",
    "                           \"polarity\": tweet.sentiment.polarity,\n",
    "                           \"subjectivity\": tweet.sentiment.subjectivity,\n",
    "                           \"sentiment\": sentiment})\n",
    "        return True\n",
    "        \n",
    "    # on failure, print the error code and do not disconnect\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # create instance of the tweepy tweet stream listener\n",
    "    listener = TweetStreamListener()\n",
    "    # set twitter keys/tokens\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)    \n",
    "    # The most exception break up the kernel in my test is ImcompleteRead. This exception handler ensures\n",
    "    # the stream to resume when breaking up by ImcompleteRead\n",
    "    while True:\n",
    "        try:\n",
    "            # create instance of the tweepy stream\n",
    "            stream = Stream(auth, listener)\n",
    "            # search twitter for keyword \"trump\"\n",
    "            stream.filter(track=['trump'])\n",
    "        except IncompleteRead:\n",
    "            continue\n",
    "        except KeyboardInterrupt:\n",
    "            # or however you want to exit this loop\n",
    "            stream.disconnect()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pipe Data From Twitter To Kibana Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Launch Elasticsearch\n",
    "   - type in *Elasticsearch* in Terminal if the environment variable has already been added to the bash profile\n",
    "   - or change the directory to the unzipped Elasticsearch folder and type *./bin/elasticsearch* to launch<br>\n",
    "   <br/>\n",
    "2. Run the python script<br>\n",
    "<br/>\n",
    "3. In a few seconds after the python script starts to run, launch Kibanada by changing the directory to the unzipped Kibana folder and type *./bin/kibana*\n",
    "    - go to 'http://localhost:5601', move to **Management**-->**Index-**->**+ Create Index Pattern** to create an index name or pattern\n",
    "    - check for the [naming convention]('https://www.elastic.co/guide/en/kibana/current/tutorial-define-index.html') of Kibana index pattern. In this example, the index name in the Python script is **logstash-a**, so I set the naming pattern in as **logstash-***. 'date' is the millisecond since epoch attribute I set in the Python script, so I choose *date* from the drag down box 'Time Filter field name'\n",
    "    - click 'Create' to create the index pattern\n",
    "    <img src='./screencapture-localhost-5601-app-kibana-naming%20pattern%20and%20datetimestamp.png' width='80%'><br>\n",
    "\n",
    "4. To transform the **date** from millisecond to readable datetimestamp, select **+ Add Scripted Field** in **Index Patterns**\n",
    "    <img src='./screencapture-localhost-5601-app-kibana-add%20DateTimeStamp.png' width='80%'><br>\n",
    "\n",
    "5. Select **Language**, **Type**, **Format** as the following graph and add *doc['date'].value* to the field of **script**\n",
    "    <img src='./screencapture-localhost-5601-app-kibana-datetime-transform-detail.png' width='80%'><br>\n",
    "\n",
    "6. See [Kibana User Guide](https://www.elastic.co/guide/en/kibana/current/getting-started.html) about how to build up the customized graphs in 'Visualize' and put them together in **Dashboard**<br>\n",
    "<br/>\n",
    "\n",
    "7. In case that I'd like to clear out the data in the elasticsearch index, run *DELETE /logstash-a* in the console of Dev Tools\n",
    "     <img src='./screencapture-localhost-5601-app-kibana-clear%20out%20data.png' width='80%'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wcml_py3.5",
   "language": "python",
   "name": "wcml_py3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
